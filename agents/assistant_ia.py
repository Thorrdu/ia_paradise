from crewai import Agent, Task, Crew, Process
from langchain.tools import BaseTool
from langchain.llms import Ollama
import subprocess
import os
import json
import psutil
import torch

# Configuration de l'agent avec acc√©l√©ration GPU
ollama_llm = Ollama(
    model="mixtral",
    temperature=0.1,
    num_gpu=1,  # Utiliser le GPU
    num_thread=8  # Utiliser 8 threads CPU
)

# Outil pour ex√©cuter des commandes syst√®me
class CommandTool(BaseTool):
    name = "command_tool"
    description = "Ex√©cute des commandes syst√®me"
    
    def _run(self, command):
        try:
            result = subprocess.run(command, shell=True, capture_output=True, text=True)
            return result.stdout
        except Exception as e:
            return f"Erreur: {str(e)}"
    
    def _arun(self, command):
        return self._run(command)

# Outil pour g√©rer les fichiers
class FileTool(BaseTool):
    name = "file_tool"
    description = "G√®re les fichiers (lecture, √©criture, liste)"
    
    def _run(self, action, path, content=None):
        try:
            if action == "list":
                files = os.listdir(path)
                return '\n'.join(files)
            elif action == "read":
                with open(path, 'r', encoding='utf-8') as f:
                    return f.read()
            elif action == "write":
                with open(path, 'w', encoding='utf-8') as f:
                    f.write(content)
                return f"Fichier {path} √©crit avec succ√®s"
            else:
                return f"Action {action} non reconnue"
        except Exception as e:
            return f"Erreur: {str(e)}"
    
    def _arun(self, action, path, content=None):
        return self._run(action, path, content)

# Outil pour surveiller les ressources syst√®me
class SystemMonitorTool(BaseTool):
    name = "system_monitor_tool"
    description = "Surveille les ressources syst√®me (CPU, RAM, GPU)"
    
    def _run(self, query_type="all"):
        try:
            if query_type == "cpu" or query_type == "all":
                cpu_usage = psutil.cpu_percent(interval=1)
                cpu_info = {"usage": cpu_usage, "cores": psutil.cpu_count()}
                
            if query_type == "ram" or query_type == "all":
                ram = psutil.virtual_memory()
                ram_info = {
                    "total": f"{ram.total / (1024**3):.2f} GB",
                    "available": f"{ram.available / (1024**3):.2f} GB",
                    "percent": ram.percent
                }
                
            if query_type == "gpu" or query_type == "all":
                if torch.cuda.is_available():
                    gpu_info = {
                        "name": torch.cuda.get_device_name(0),
                        "available": True,
                        "memory_allocated": f"{torch.cuda.memory_allocated(0) / (1024**3):.2f} GB",
                        "memory_reserved": f"{torch.cuda.memory_reserved(0) / (1024**3):.2f} GB"
                    }
                else:
                    gpu_info = {"available": False}
            
            result = {}
            if query_type == "cpu" or query_type == "all":
                result["cpu"] = cpu_info
            if query_type == "ram" or query_type == "all":
                result["ram"] = ram_info
            if query_type == "gpu" or query_type == "all":
                result["gpu"] = gpu_info
                
            return json.dumps(result, indent=2)
        except Exception as e:
            return f"Erreur: {str(e)}"
    
    def _arun(self, query_type="all"):
        return self._run(query_type)

if __name__ == "__main__":
    print("ü§ñ D√©marrage de l'Assistant IA...")
    
    # V√©rifier si Ollama est install√© et en cours d'ex√©cution
    try:
        ollama_version = subprocess.run(["ollama", "version"], capture_output=True, text=True)
        print(f"‚úÖ Ollama d√©tect√©: {ollama_version.stdout.strip()}")
    except:
        print("‚ùå Ollama n'est pas install√© ou n'est pas dans le PATH")
        print("Veuillez installer Ollama depuis: https://ollama.com/download/windows")
        exit(1)
    
    # V√©rifier si le mod√®le mixtral est disponible
    try:
        models = subprocess.run(["ollama", "list"], capture_output=True, text=True)
        if "mixtral" not in models.stdout:
            print("‚¨áÔ∏è Le mod√®le mixtral n'est pas disponible. T√©l√©chargement en cours...")
            subprocess.run(["ollama", "pull", "mixtral"], check=True)
        print("‚úÖ Mod√®le mixtral disponible")
    except Exception as e:
        print(f"‚ùå Erreur lors de la v√©rification/t√©l√©chargement du mod√®le: {str(e)}")
        exit(1)
    
    # Cr√©ation de l'agent assistant syst√®me
    assistant = Agent(
        role="Assistant Syst√®me Avanc√©",
        goal="Aider √† g√©rer l'ordinateur et automatiser des t√¢ches complexes",
        backstory="Je suis un assistant IA puissant con√ßu pour l'automatisation et la gestion des t√¢ches syst√®me sur un ordinateur haute performance.",
        verbose=True,
        tools=[CommandTool(), FileTool(), SystemMonitorTool()],
        llm=ollama_llm
    )
    
    # Menu interactif
    print("\n=== ü§ñ ASSISTANT IA - MENU DES T√ÇCHES ===")
    print("1. Analyser les performances du syst√®me")
    print("2. Nettoyer les fichiers temporaires")
    print("3. V√©rifier les mises √† jour logicielles")
    print("4. Analyser l'utilisation du disque")
    print("5. Mode conversation libre")
    print("======================================")
    
    choice = input("Choisissez une option (1-5): ")
    
    if choice == "1":
        task = Task(
            description="Analyser les performances actuelles du syst√®me et g√©n√©rer un rapport complet incluant CPU, RAM, GPU, et recommandations d'optimisation.",
            expected_output="Rapport de performance du syst√®me",
            agent=assistant
        )
    elif choice == "2":
        task = Task(
            description="Identifier et nettoyer les fichiers temporaires qui prennent de l'espace inutilement sur le syst√®me.",
            expected_output="Rapport sur les fichiers nettoy√©s et l'espace lib√©r√©",
            agent=assistant
        )
    elif choice == "3":
        task = Task(
            description="V√©rifier les mises √† jour disponibles pour le syst√®me d'exploitation et les logiciels principaux install√©s.",
            expected_output="Liste des mises √† jour disponibles",
            agent=assistant
        )
    elif choice == "4":
        task = Task(
            description="Analyser l'utilisation du disque et identifier les dossiers et fichiers qui occupent le plus d'espace.",
            expected_output="Rapport sur l'utilisation du disque",
            agent=assistant
        )
    elif choice == "5":
        user_query = input("\nQue souhaitez-vous demander √† l'Assistant IA? ")
        task = Task(
            description=f"R√©pondre √† la demande de l'utilisateur: {user_query}",
            expected_output="R√©ponse √† la demande de l'utilisateur",
            agent=assistant
        )
    else:
        print("‚ùå Option invalide, utilisation de l'option 1 par d√©faut.")
        task = Task(
            description="Analyser les performances actuelles du syst√®me et g√©n√©rer un rapport.",
            expected_output="Rapport de performance du syst√®me",
            agent=assistant
        )
    
    # Cr√©ation de l'√©quipe (Crew)
    crew = Crew(
        agents=[assistant],
        tasks=[task],
        verbose=2,
        process=Process.sequential  # Ex√©cution s√©quentielle des t√¢ches
    )
    
    # Ex√©cution de l'√©quipe
    print("\nüöÄ Ex√©cution de la t√¢che... (cela peut prendre quelques minutes)")
    result = crew.kickoff()
    
    print("\n=== üìã R√âSULTAT DE LA T√ÇCHE ===")
    print(result)
    print("===============================")
    
    input("\nAppuyez sur Entr√©e pour quitter...") 