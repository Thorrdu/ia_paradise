# üöÄ Plan d'Action V2 : Paradis IA Haute Performance

Ce document contient les √©tapes optimis√©es pour installer un √©cosyst√®me d'intelligence artificielle 100% gratuit et local sur un ordinateur puissant : **Windows 11 avec 32-64 Go de RAM et une GeForce RTX 4060**.

## üìã Aper√ßu des Capacit√©s Avanc√©es

Avec cette configuration mat√©rielle haut de gamme, notre Paradis IA pourra :
- Ex√©cuter plusieurs mod√®les IA de grande taille simultan√©ment
- Exploiter l'acc√©l√©ration GPU pour des performances optimales
- G√©rer des projets de d√©veloppement web/PHP complexes
- Assurer une r√©ponse rapide et fluide des agents IA

## üîß Configuration Mat√©rielle Optimale D√©tect√©e
- **Syst√®me d'exploitation** : Windows 11
- **RAM** : 32-64 Go
- **GPU** : NVIDIA GeForce RTX 4060 (8 Go VRAM)
- **Optimisations possibles** : Acc√©l√©ration CUDA, mod√®les quantifi√©s pour GPU

## üìÅ Structure Organisationnelle du Projet

```
Paradis_IA/
‚îú‚îÄ‚îÄ .env                        # Variables d'environnement
‚îú‚îÄ‚îÄ README.md                   # Documentation principale
‚îú‚îÄ‚îÄ collecter_infos_systeme.ps1 # Script diagnostic syst√®me
‚îú‚îÄ‚îÄ lancer_paradis_ia.ps1       # Script de lancement principal
‚îÇ
‚îú‚îÄ‚îÄ modeles/                    # R√©pertoire des mod√®les IA
‚îÇ   ‚îî‚îÄ‚îÄ modelfiles/             # Configurations personnalis√©es de mod√®les
‚îÇ
‚îú‚îÄ‚îÄ agents/                     # Agents IA sp√©cialis√©s
‚îÇ   ‚îú‚îÄ‚îÄ assistant_ia.py         # Agent principal d'automatisation
‚îÇ   ‚îú‚îÄ‚îÄ dev_php_agent.py        # Agent sp√©cialis√© PHP
‚îÇ   ‚îî‚îÄ‚îÄ system_agent.py         # Agent de gestion syst√®me
‚îÇ
‚îú‚îÄ‚îÄ config/                     # Fichiers de configuration
‚îÇ   ‚îú‚îÄ‚îÄ ollama_config.json      # Configuration d'Ollama
‚îÇ   ‚îî‚îÄ‚îÄ tabby_config.json       # Configuration de TabbyML
‚îÇ
‚îî‚îÄ‚îÄ scripts/                    # Scripts utilitaires
    ‚îú‚îÄ‚îÄ setup_environment.ps1   # Installation des pr√©requis
    ‚îú‚îÄ‚îÄ gpu_optimization.ps1    # Optimisation pour GPU NVIDIA
    ‚îî‚îÄ‚îÄ test_performance.ps1    # Tests de performance
```

## üîç √âtape 0 : Collecte et Analyse du Syst√®me

Cette √©tape est d√©j√† r√©solue car nous connaissons la configuration : Windows 11, 32-64 Go RAM, RTX 4060.

## üõ†Ô∏è √âtape 1 : Installation des Pr√©requis Optimis√©s

### Python avec Support GPU
```powershell
# Installation Python 3.10+ (version optimale pour compatibilit√© CUDA)
Start-Process "https://www.python.org/downloads/windows/"

# V√©rification
python --version
pip --version

# Installation des packages optimis√©s GPU
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install tensorflow-gpu
```

### Ollama avec Support CUDA
```powershell
# T√©l√©charger et installer Ollama
Start-Process "https://ollama.com/download/windows"

# V√©rifier l'installation
ollama --version

# Configurer pour utilisation GPU
$ollamaConfigPath = "$env:USERPROFILE\.ollama\config.json"
$ollamaConfig = @{
    "gpu" = $true
    "cuda" = $true
}
$ollamaConfig | ConvertTo-Json | Out-File -FilePath $ollamaConfigPath
```

### LM Studio (Configuration GPU)
```powershell
# T√©l√©charger et installer LM Studio
Start-Process "https://lmstudio.ai/"

# Apr√®s installation, configurer pour utilisation GPU dans l'interface
```

### Docker avec Support WSL2 et GPU
```powershell
# Activer WSL2
wsl --install

# Installer Docker Desktop
Start-Process "https://www.docker.com/products/docker-desktop/"

# Apr√®s installation, activer l'int√©gration GPU dans les param√®tres de Docker Desktop
```

### CUDA Toolkit et cuDNN
```powershell
# Installer CUDA Toolkit pour optimiser les performances IA
Start-Process "https://developer.nvidia.com/cuda-downloads"

# Installer cuDNN pour acc√©l√©rer les r√©seaux de neurones
Start-Process "https://developer.nvidia.com/cudnn"
```

## ü§ñ √âtape 2 : Mise en Place des IA pour le Contr√¥le de l'Ordinateur

### CrewAI (Optimis√© pour Hautes Performances)
```powershell
# Cr√©er un environnement virtuel Python
python -m venv crew_env
.\crew_env\Scripts\Activate

# Installer CrewAI avec d√©pendances GPU
pip install crewai langchain
pip install transformers accelerate bitsandbytes
```

### Installation des Mod√®les IA Haut de Gamme
```powershell
# Installer Mixtral 8x7B (mod√®le de classe mondiale)
ollama pull mixtral

# Installer Dolphin Mixtral (instructions suivies optimis√©es)
ollama pull dolphin-mixtral

# Installer CodeLlama 34B (mod√®le de code haute qualit√©)
ollama pull codellama:34b-instruct-q5_K_M

# V√©rifier l'acc√©l√©ration GPU
ollama run dolphin-mixtral "V√©rifie si tu utilises bien l'acc√©l√©ration GPU. R√©ponds uniquement par Oui ou Non."
```

### Script d'Automatisation Avanc√©

Cr√©er le fichier `agents/assistant_ia.py` :
```python
from crewai import Agent, Task, Crew, Process
from langchain.tools import BaseTool
from langchain.llms import Ollama
import subprocess
import os
import json
import psutil
import torch

# Configuration de l'agent avec acc√©l√©ration GPU
ollama_llm = Ollama(
    model="mixtral",
    temperature=0.1,
    num_gpu=1,  # Utiliser le GPU
    num_thread=8  # Utiliser 8 threads CPU
)

# Outil pour ex√©cuter des commandes syst√®me
class CommandTool(BaseTool):
    name = "command_tool"
    description = "Ex√©cute des commandes syst√®me"
    
    def _run(self, command):
        try:
            result = subprocess.run(command, shell=True, capture_output=True, text=True)
            return result.stdout
        except Exception as e:
            return f"Erreur: {str(e)}"
    
    def _arun(self, command):
        return self._run(command)

# Outil pour g√©rer les fichiers
class FileTool(BaseTool):
    name = "file_tool"
    description = "G√®re les fichiers (lecture, √©criture, liste)"
    
    def _run(self, action, path, content=None):
        try:
            if action == "list":
                files = os.listdir(path)
                return '\n'.join(files)
            elif action == "read":
                with open(path, 'r', encoding='utf-8') as f:
                    return f.read()
            elif action == "write":
                with open(path, 'w', encoding='utf-8') as f:
                    f.write(content)
                return f"Fichier {path} √©crit avec succ√®s"
            else:
                return f"Action {action} non reconnue"
        except Exception as e:
            return f"Erreur: {str(e)}"
    
    def _arun(self, action, path, content=None):
        return self._run(action, path, content)

# Outil pour surveiller les ressources syst√®me
class SystemMonitorTool(BaseTool):
    name = "system_monitor_tool"
    description = "Surveille les ressources syst√®me (CPU, RAM, GPU)"
    
    def _run(self, query_type="all"):
        try:
            if query_type == "cpu" or query_type == "all":
                cpu_usage = psutil.cpu_percent(interval=1)
                cpu_info = {"usage": cpu_usage, "cores": psutil.cpu_count()}
                
            if query_type == "ram" or query_type == "all":
                ram = psutil.virtual_memory()
                ram_info = {
                    "total": f"{ram.total / (1024**3):.2f} GB",
                    "available": f"{ram.available / (1024**3):.2f} GB",
                    "percent": ram.percent
                }
                
            if query_type == "gpu" or query_type == "all":
                if torch.cuda.is_available():
                    gpu_info = {
                        "name": torch.cuda.get_device_name(0),
                        "available": True,
                        "memory_allocated": f"{torch.cuda.memory_allocated(0) / (1024**3):.2f} GB",
                        "memory_reserved": f"{torch.cuda.memory_reserved(0) / (1024**3):.2f} GB"
                    }
                else:
                    gpu_info = {"available": False}
            
            result = {}
            if query_type == "cpu" or query_type == "all":
                result["cpu"] = cpu_info
            if query_type == "ram" or query_type == "all":
                result["ram"] = ram_info
            if query_type == "gpu" or query_type == "all":
                result["gpu"] = gpu_info
                
            return json.dumps(result, indent=2)
        except Exception as e:
            return f"Erreur: {str(e)}"
    
    def _arun(self, query_type="all"):
        return self._run(query_type)

# Cr√©ation de l'agent assistant syst√®me
assistant = Agent(
    role="Assistant Syst√®me Avanc√©",
    goal="Aider √† g√©rer l'ordinateur et automatiser des t√¢ches complexes",
    backstory="Je suis un assistant IA puissant con√ßu pour l'automatisation et la gestion des t√¢ches syst√®me sur un ordinateur haute performance.",
    verbose=True,
    tools=[CommandTool(), FileTool(), SystemMonitorTool()],
    llm=ollama_llm
)

# Exemple de t√¢che : analyser les performances du syst√®me
task = Task(
    description="Analyser les performances actuelles du syst√®me et g√©n√©rer un rapport",
    expected_output="Rapport de performance du syst√®me",
    agent=assistant
)

# Cr√©ation de l'√©quipe (Crew)
crew = Crew(
    agents=[assistant],
    tasks=[task],
    verbose=2,
    process=Process.sequential  # Ex√©cution s√©quentielle des t√¢ches
)

# Ex√©cution de l'√©quipe
result = crew.kickoff()
print(result)
```

## üíª √âtape 3 : Installation des IA pour le D√©veloppement (Optimis√©es)

### Mod√®les de Code Haute Performance
```powershell
# Code Llama 34B (version haute performance)
ollama pull codellama:34b-instruct-q5_K_M

# DeepSeek Coder (sp√©cialis√© PHP et mod√®le de premier ordre)
ollama pull deepseek-coder:33b-instruct-q5_K_M

# Tester les mod√®les de code
ollama run codellama:34b-instruct-q5_K_M "G√©n√®re une classe PHP compl√®te pour un gestionnaire de bases de donn√©es avec PDO"
```

### TabbyML avec Acc√©l√©ration GPU
```powershell
# Installer TabbyML
curl -fsSL https://get.tabbyml.com/install.sh | bash

# Lancer TabbyML avec mod√®le haut de gamme et acc√©l√©ration GPU
tabby serve --model TabbyML/DeepseekCoder-33B-instruct-q5_K_M --device cuda

# Installer l'extension TabbyML pour VS Code ou PHPStorm
```

### OpenDevin avec configuration avanc√©e
```powershell
# Cloner le d√©p√¥t
git clone https://github.com/OpenDevin/OpenDevin.git
cd OpenDevin

# Configurer OpenDevin pour utilisation GPU
$env:OPENDEVIN_GPU_ACCELERATION = "true"
$env:OPENDEVIN_MODEL = "codellama:34b-instruct-q5_K_M"

# D√©marrer avec Docker et acc√©l√©ration GPU
docker compose -f docker-compose.yml -f docker-compose.gpu.yml up
```

## üîß √âtape 4 : Configuration Avanc√©e pour GPU RTX 4060

### Optimisation des Mod√®les pour le GPU
```powershell
# Cr√©er un dossier pour les Modelfiles personnalis√©s
mkdir -p modeles/modelfiles

# Cr√©er un Modelfile optimis√© pour Mixtral avec configuration GPU
@"
FROM mixtral
PARAMETER num_ctx 16384
PARAMETER num_gpu 1
PARAMETER num_thread 8
PARAMETER temperature 0.7
PARAMETER stop "<|im_end|>"
PARAMETER stop "</answer>"
"@ | Out-File -FilePath "modeles/modelfiles/mixtral-optimized"

# Cr√©er et ex√©cuter le mod√®le optimis√©
ollama create mixtral-optimized -f modeles/modelfiles/mixtral-optimized
```

### Optimisation du GPU avec CUDA
```powershell
# Cr√©er le script pour optimisation GPU
@"
Write-Host "üöÄ Optimisation GPU RTX 4060 pour IA" -ForegroundColor Cyan

# V√©rifier la pr√©sence de CUDA
if (-not (Test-Path "C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA")) {
    Write-Host "‚ùå CUDA non d√©tect√©. Installez CUDA Toolkit depuis:" -ForegroundColor Red
    Write-Host "https://developer.nvidia.com/cuda-downloads" -ForegroundColor Yellow
    Exit
}

# Optimiser le Shader Cache pour DirectX
Set-ItemProperty -Path "HKLM:\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\Advanced" -Name "DisableShaderCache" -Value 0 -Type DWord

# Activer le Game Mode pour optimiser les performances
Set-ItemProperty -Path "HKCU:\Software\Microsoft\GameBar" -Name "AllowAutoGameMode" -Value 1 -Type DWord
Set-ItemProperty -Path "HKCU:\Software\Microsoft\GameBar" -Name "AutoGameModeEnabled" -Value 1 -Type DWord

# Configurer NVIDIA pour performances maximales
nvidia-smi -pm 1
nvidia-smi --auto-boost-default=0
nvidia-smi -ac 3004,1708

Write-Host "‚úÖ Optimisation GPU termin√©e!" -ForegroundColor Green
"@ | Out-File -FilePath "scripts/gpu_optimization.ps1"
```

## üîÑ √âtape 5 : Configuration Multi-Agents Avanc√©e

### Script de Lancement avec Menu √âtendu

Cr√©er le fichier `lancer_paradis_ia.ps1` :
```powershell
# Script de lancement du Paradis IA - Version haute performance
Write-Host "üöÄ Lancement du Paradis IA - Configuration haute performance" -ForegroundColor Cyan

# Fonction pour v√©rifier si un processus est en cours d'ex√©cution
function Is-ProcessRunning {
    param ($processName)
    return (Get-Process | Where-Object { $_.ProcessName -eq $processName }) -ne $null
}

# V√©rifier et lancer Ollama si n√©cessaire
if (-not (Is-ProcessRunning "ollama")) {
    Write-Host "üì¶ D√©marrage d'Ollama (mode GPU)..." -ForegroundColor Yellow
    Start-Process "ollama" -WindowStyle Hidden
    Start-Sleep -Seconds 3
}

# Activation de l'environnement virtuel CrewAI
Write-Host "ü§ñ Activation de l'environnement CrewAI..." -ForegroundColor Yellow
.\crew_env\Scripts\Activate

# Fonction pour afficher la disponibilit√© GPU
function Show-GPUStatus {
    Write-Host "`n===== üñ•Ô∏è STATUT GPU NVIDIA =====`n" -ForegroundColor Cyan
    try {
        $gpuInfo = nvidia-smi --query-gpu=name,memory.used,memory.total,temperature.gpu --format=csv,noheader
        Write-Host $gpuInfo -ForegroundColor Green
    } catch {
        Write-Host "Impossible d'obtenir les informations GPU" -ForegroundColor Red
    }
    Write-Host ""
}

# Menu interactif avanc√©
function Show-Menu {
    Write-Host "`n=== ü§ñ PARADIS IA - MENU PRINCIPAL (HAUTE PERFORMANCE) ü§ñ ===" -ForegroundColor Cyan
    
    Show-GPUStatus
    
    Write-Host "=== üí¨ ASSISTANTS CONVERSATIONNELS ===" -ForegroundColor Magenta
    Write-Host "1. üß† Mixtral 8x7B (Mod√®le haut de gamme)" -ForegroundColor Green
    Write-Host "2. üê¨ Dolphin Mixtral (Instructions optimis√©es)" -ForegroundColor Green
    
    Write-Host "`n=== üíª ASSISTANTS D√âVELOPPEMENT ===" -ForegroundColor Magenta
    Write-Host "3. üë®‚Äçüíª Code Llama 34B (Code haute qualit√©)" -ForegroundColor Green
    Write-Host "4. üîç DeepSeek Coder 33B (Sp√©cialiste PHP)" -ForegroundColor Green
    Write-Host "5. üìã TabbyML GPU (Auto-compl√©tion)" -ForegroundColor Green
    Write-Host "6. üöÄ OpenDevin (D√©veloppement autonome)" -ForegroundColor Green
    
    Write-Host "`n=== üõ†Ô∏è AGENTS & OUTILS ===" -ForegroundColor Magenta
    Write-Host "7. üîß Assistant IA (Agent syst√®me)" -ForegroundColor Green
    Write-Host "8. üìä Rapport de performances" -ForegroundColor Green
    Write-Host "9. ‚öôÔ∏è Optimisation GPU" -ForegroundColor Green
    
    Write-Host "`n10. ‚ùå Quitter" -ForegroundColor Red
    Write-Host "==================================" -ForegroundColor Cyan
}

# Boucle principale
do {
    Show-Menu
    $choice = Read-Host "Choisissez une option (1-10)"
    
    switch ($choice) {
        "1" { ollama run mixtral-optimized }
        "2" { ollama run dolphin-mixtral }
        "3" { ollama run codellama:34b-instruct-q5_K_M }
        "4" { ollama run deepseek-coder:33b-instruct-q5_K_M }
        "5" { 
            Write-Host "Lancement de TabbyML avec acc√©l√©ration GPU..." -ForegroundColor Yellow
            tabby serve --model TabbyML/DeepseekCoder-33B-instruct-q5_K_M --device cuda 
        }
        "6" { 
            Set-Location OpenDevin
            $env:OPENDEVIN_GPU_ACCELERATION = "true"
            docker compose -f docker-compose.yml -f docker-compose.gpu.yml up
            Set-Location ..
        }
        "7" { 
            Write-Host "Lancement de l'Assistant IA..." -ForegroundColor Yellow
            python agents/assistant_ia.py 
        }
        "8" { 
            Write-Host "G√©n√©ration du rapport de performances..." -ForegroundColor Yellow
            python scripts/test_performance.ps1
        }
        "9" { 
            Write-Host "Optimisation du GPU..." -ForegroundColor Yellow
            .\scripts\gpu_optimization.ps1 
        }
        "10" { 
            Write-Host "üëã Fermeture du Paradis IA..." -ForegroundColor Cyan
            break 
        }
        default { Write-Host "‚ùå Option invalide, veuillez r√©essayer." -ForegroundColor Red }
    }
} while ($choice -ne "10")

# D√©sactivation de l'environnement virtuel
deactivate
```

## üìä √âtape 6 : Tests de Performance et Optimisation

### Script de Test de Performance

Cr√©er le fichier `scripts/test_performance.ps1` :
```powershell
# Script de test de performance du Paradis IA
Write-Host "üìä Test de performance des mod√®les IA" -ForegroundColor Cyan

$outputFile = "rapport_performance.txt"

# Tester chaque mod√®le avec un prompt standard
$testPrompt = "Explique le concept de programmation orient√©e objet en PHP en 5 phrases concises."

# Fonction pour tester un mod√®le
function Test-Model {
    param($modelName, $modelParam)
    
    Write-Host "Test du mod√®le $modelName..." -ForegroundColor Yellow
    
    # Mesurer le temps de d√©marrage
    $startTime = Get-Date
    $result = ollama run $modelParam -q "$testPrompt"
    $endTime = Get-Date
    $duration = ($endTime - $startTime).TotalSeconds
    
    # √âcrire les r√©sultats
    "=== Test de $modelName ===" | Out-File -FilePath $outputFile -Append
    "Temps de r√©ponse: $duration secondes" | Out-File -FilePath $outputFile -Append
    "R√©ponse: $result" | Out-File -FilePath $outputFile -Append
    "`n" | Out-File -FilePath $outputFile -Append
    
    return @{
        Model = $modelName
        Duration = $duration
        ResponseLength = $result.Length
    }
}

# Pr√©parer le fichier de sortie
"RAPPORT DE PERFORMANCE DES MOD√àLES IA" | Out-File -FilePath $outputFile
"Date: $(Get-Date -Format 'yyyy-MM-dd HH:mm:ss')" | Out-File -FilePath $outputFile -Append
"Syst√®me: Windows 11, RTX 4060, $(Get-CimInstance Win32_ComputerSystem).TotalPhysicalMemory / 1GB GB RAM" | Out-File -FilePath $outputFile -Append
"`n" | Out-File -FilePath $outputFile -Append

# Tester tous les mod√®les
$results = @()
$results += Test-Model -modelName "Mixtral 8x7B" -modelParam "mixtral-optimized"
$results += Test-Model -modelName "Dolphin Mixtral" -modelParam "dolphin-mixtral"
$results += Test-Model -modelName "CodeLlama 34B" -modelParam "codellama:34b-instruct-q5_K_M"
$results += Test-Model -modelName "DeepSeek Coder" -modelParam "deepseek-coder:33b-instruct-q5_K_M"

# G√©n√©rer un r√©sum√©
"=== R√âSUM√â DES PERFORMANCES ===" | Out-File -FilePath $outputFile -Append
$results | Sort-Object Duration | ForEach-Object {
    "$($_.Model): $($_.Duration) secondes, $($_.ResponseLength) caract√®res" | Out-File -FilePath $outputFile -Append
}

Write-Host "‚úÖ Rapport de performance g√©n√©r√© dans $outputFile" -ForegroundColor Green
```

## üèÅ √âtape 7 : Premier Lancement et Tests de Validation

```powershell
# Ex√©cution du script d'optimisation GPU
.\scripts\gpu_optimization.ps1

# Lancement du Paradis IA
.\lancer_paradis_ia.ps1
```

## üìö Ressources et Documentation

- [Optimisation NVIDIA pour l'IA](https://developer.nvidia.com/deep-learning-performance-training-inference)
- [Guide d'optimisation CUDA pour Windows 11](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html)
- [Documentation Ollama pour GPU](https://github.com/ollama/ollama/blob/main/docs/gpu.md)
- [Documentation CrewAI pour acc√©l√©ration](https://docs.crewai.com/how-to/Using-LLMs/)

## ‚ö†Ô∏è R√©solution des Probl√®mes Courants

### CUDA Out of Memory
Si vous rencontrez cette erreur, essayez de diminuer la taille du contexte ou d'utiliser un mod√®le quantifi√© plus l√©ger :
```powershell
# Utiliser un mod√®le mieux quantifi√©
ollama pull codellama:34b-instruct-q6_K
```

### Performances GPU sous-optimales
Assurez-vous que les pilotes NVIDIA sont √† jour et que votre syst√®me n'ex√©cute pas d'autres processus gourmands en GPU.

## üîÆ Prochaines Am√©liorations Possibles

- Int√©gration de mod√®les multimodaux (texte + image) comme LLaVA-NeXT
- Cr√©ation d'un tableau de bord web pour le monitoring des performances
- Fine-tuning des mod√®les pour des t√¢ches sp√©cifiques en PHP/d√©veloppement
- Int√©gration avec des API externes via LangGraph 